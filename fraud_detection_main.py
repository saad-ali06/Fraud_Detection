# -*- coding: utf-8 -*-
"""Fraud Detection main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1unCaHOoahOiJ7f_AOO3hyxDoqhQPLb-s
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# Add this line to display plots inline in Jupyter Notebook
# %matplotlib inline
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder
from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score,f1_score, roc_curve
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler, SMOTE
from imblearn.combine import SMOTEENN
from sklearn.model_selection import GridSearchCV

pd.set_option('display.max_columns', 25)  # Display all columns
pd.set_option('display.max_rows', 1000)     # Display all rows

"""We are using this dataset from kaggle: https://www.kaggle.com/datasets/dermisfit/fraud-transactions-dataset?select=fraudTrain.csv

This dataset is divied into train and test, and has the same 23 columns..<br>
train : 1.30m <br>
test : 556k
"""

df_train = pd.read_csv('/content/drive/MyDrive/Fraud Detection/DataSet/archive (3)/fraudTrain.csv')

"""# Data Exploration:"""

df_train.head()

df_train.describe()

df_train.info()

df_train.isna().sum()

"""Does't have any empty value.
If this DataSet has any empty values we use this code:
"""

# df['Age'].fillna(df['Age'].mean(), inplace=True)
# df['Age'] = df['Age'].round()
# df['Cabin'].fillna('missing',inplace=True)
# df['Embarked'].fillna('missing',inplace=True)
"""
This code filled numerical empty value with mean of the whole column and
filled string type values with 'missing' string which represents category of its own.
"""

ax = sns.countplot(x='is_fraud', data=df_train)
print(df_train['is_fraud'].value_counts())

lenght_Fraud =  len(df_train[df_train['is_fraud']==1])
lenght_notFraud = len(df_train[df_train['is_fraud']==0])
total_lenght = len(df_train['is_fraud'])

print(f'Percentage of Fraud ratio in DataSet is {lenght_Fraud/total_lenght*100:.2f}% Fraud Representation and {lenght_notFraud/total_lenght*100:.2f}% Not Fraud Representation')

"""It clearly show class Imbalance. To overcome this problem we going to use
class_weight='balanced' from sklearn library.
which uses this formula

<img src="./class_weight_inp_image.png">

# Data Preprocessing:
"""

feature = df_train.columns
feature

"""Now, finding any outliers."""

# Create a histogram
plt.figure(figsize=(12, 6))
sns.histplot(x='city', y='amt', data=df_train, bins=30, kde=True, palette='viridis')
plt.title('Histogram of Amount by City')
plt.xlabel('City')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(x='state', y='amt', data=df_train, bins=30, kde=True, palette='viridis')
plt.title('Histogram of Amount by State')
plt.xlabel('State')
plt.ylabel('Frequency')
plt.show()

df_train[df_train['amt']>= 3000]['is_fraud'].value_counts()

"""We have nearly 13m rows and nearly few 100s so them are above 3000 and all are not fraud.
so, we going to remove them.
"""

df_train = df_train[df_train['amt'] < 3000]

"""Drawing scatterplots based on Location of user and merchant"""

#  df_train is DataFrame
#  creating scatterplot based on user location which latitude and longitude.
plt.figure(figsize=(12, 8))
sns.scatterplot(x='long', y='lat', data=df_train, hue='amt', palette='viridis', alpha=0.5)
plt.title('Scatter Plot of Longitude and Latitude of User by Amount')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.legend(title='Amount', loc='upper right')
plt.show()

#  creating scatterplot based on merchant location.
plt.figure(figsize=(12, 8))
sns.scatterplot(x='merch_long', y='merch_lat', data=df_train, hue='state', palette='viridis', alpha=0.5)
plt.title('Scatter Plot of Longitude and Latitude of Merchant by State')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.legend(title='City', loc='upper right')
plt.show()

"""These graph show that 5 users which fall in logitude less than -130 and spend money between 1000 to 2000.<br>
These graph also show that there state are near AK and HI which is in general false indication.<br>
so, We going to check them.
"""

print(f'Number of Credit Card Unique Customer Fall under Above criteria {df_train[df_train["long"] < -130]["first"].unique()}')
print(df_train[df_train["long"] < -130]["is_fraud"].value_counts())
print(df_train[df_train["long"] < -130]["state"].value_counts())

"""Althrough it considering 13 million row it is a fraction of data shown here.<br>
But faking location must be consider into fraudulent account so we are going to remove the row which is not Fraud.<br>
So, Our model learn the pattern of miss placed location as Fraud.
"""

# number of rows before removed data
df_train.shape

# number of rows after data removed
filtered_rows = df_train[(df_train["long"] < -130) & (df_train['is_fraud']==0)]
df_train.drop(filtered_rows.index, inplace=True)
df_train.shape

"""Dropping Relevant features"""

feature

df_train.drop(columns=['Unnamed: 0', 'cc_num','first', 'last', 'unix_time','trans_num'], inplace=True)
feature = df_train.columns
feature

object_columns = df_train.select_dtypes(include='object').columns
int_columns = df_train.select_dtypes(include=['int64','float64']).columns
print('Object Type columns are:\n',object_columns := list(object_columns))
print('Int Type columns are:\n',int_columns := list(int_columns))

"""Label Encoding of list ['merchant', 'category', 'street', 'city', 'state', 'job']<br>
So that category convert into numerical form
"""

print(feature)
desired_order =['merchant', 'category', 'street', 'city', 'state', 'job','trans_date_trans_time', 'amt',
                'gender', 'zip', 'lat', 'long', 'city_pop','dob', 'merch_lat', 'merch_long', 'is_fraud']
df_train = df_train[desired_order]
df_train

df_train

"""Make a column Transformer for endcoding."""

column_trans = make_column_transformer(
    (OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1), ['merchant', 'category', 'street', 'city', 'state', 'job']),
    remainder='passthrough')

X = column_trans.fit_transform(df_train)

X = pd.DataFrame(X,columns=df_train.columns)
X.head()

df_train.head()

df_train = X

"""We are going to use pipe line from sklearn lib to scalar and encode our data.<br>
The main reason for doing because as we know our dataset is divide into train and test files.<br>
So, when we load our dataset we don't to perform this feature prcessing and engineering on test data from scrash.<br>
we pass test data to pipline and our test dataset will ready to use.

# Feature Engineering:
"""

# Extract time-based features
df_train['trans_hour'] = pd.to_datetime(df_train['trans_date_trans_time']).dt.hour
df_train['trans_day_of_week'] = pd.to_datetime(df_train['trans_date_trans_time']).dt.dayofweek

df_train.head()

"""Converting dob into Age column"""

from datetime import datetime

# 'dob' is the column containing date of birth
df_train['dob'] = pd.to_datetime(df_train['dob'])

# Calculate age based on the current date
current_date = datetime.now()
df_train['age'] = (current_date - df_train['dob']).dt.days // 365

# Display the updated DataFrame
print(df_train[['dob', 'age']])

print(df_train['trans_day_of_week'].unique(),df_train['trans_hour'].unique())

df_train.drop(columns=['trans_date_trans_time', 'dob'], inplace=True)
feature = df_train.columns
feature

"""Creating a new column where High Risk Merchants denoted by 1 else 0."""

HighRiskMerchant = df_train[df_train['is_fraud'] == 1]['merchant'].value_counts()
HighRiskMerchant = HighRiskMerchant[HighRiskMerchant > 20].index.tolist()

# Create 'high_risk_merchant' column
df_train['high_risk_merchant'] = np.where(df_train['merchant'].isin(HighRiskMerchant), 1, 0)

df_train['high_risk_merchant'].value_counts()

"""Now adding High risk states, cities, High Risk week days, high risk job( finding people with similar jobs commit fraud )"""

HighRiskCity =  df_train[df_train['is_fraud'] == 1]['city'].value_counts().index.tolist()
df_train['high_risk_city'] = np.where(df_train['city'].isin(HighRiskCity) & df_train['city_pop']>1000, 1, 0)
HighRiskState =  df_train[df_train['is_fraud'] == 1]['state'].value_counts().index.tolist()
df_train['high_risk_state'] =  np.where(df_train['state'].isin(HighRiskState) , 1, 0)
HighRiskJob =  df_train[df_train['is_fraud'] == 1]['job'].value_counts()
HighRiskJob = HighRiskJob[HighRiskJob > 10].index.tolist()
df_train['high_risk_job'] = np.where(df_train['job'].isin(HighRiskJob), 1, 0)

feature = df_train.columns
print(feature)
object_columns = df_train.select_dtypes(include='object').columns
int_columns = df_train.select_dtypes(include=['int32','int64','float32','float64']).columns
print('Object Type columns are:\n',object_columns := list(object_columns))
print('Int Type columns are:\n',int_columns := list(int_columns))

object_columns.remove('gender')
object_columns

df_train[object_columns] = df_train[object_columns].astype(float)
object_columns = df_train.select_dtypes(include='object').columns
int_columns = df_train.select_dtypes(include=['int32','int64','float32','float64']).columns
print('Object Type columns are:\n',object_columns := list(object_columns))
print('Int Type columns are:\n',int_columns := list(int_columns))

int_columns.remove('is_fraud')

df_train.head()

"""These columns ['merchant', 'category', 'street', 'city', 'state', 'job'] already Ordinal encodered

# Model Selection

Using Logistc Regression from Sklearn because it is a classification problem.
"""

from sklearn.linear_model import LogisticRegression
lg = LogisticRegression()

"""# Model Training

Data is already split in Two file Test and Train.<br>
But first we check accuracy to train dataset and then on Test dataset which give us the fair
Model evalutaion about overfitting and underfitting.

### Make Column Transformer for scaling and encoding columns Scaling and Encoding.
So, when next batch of data came in our case test data comes it encoded accordingly.
"""

# Column Transformer 2
column_trans_2 = make_column_transformer(
    (StandardScaler(), int_columns),
    (OneHotEncoder(), ['gender'])
)

# Combine transformer into a single pipeline
pipe = make_pipeline(column_trans_2,LogisticRegression(max_iter=10000))

# Separate target variable and features
y_train = df_train['is_fraud']
X_train = df_train.drop(columns='is_fraud')
print(y_train.shape)
print(X_train.shape)

pipe.fit(X_train,y_train)

"""# Cross Validation Score

Commented because it take too much time and dataset is too large.
"""

# print(cross_val_score(pipe, X_train, y_train, cv=5, scoring='precision').mean())
# print(cross_val_score(pipe, X_train, y_train, cv=5, scoring='recall').mean())
# cross_val_score(pipe, X_train, y_train, cv=5, scoring='f1').mean()


# numpy.core._exceptions._ArrayMemoryError: Unable to allocate 158. MiB for an array with
# shape (1033348, 20) and data type float64

"""# Model Evaluation:

We Evaluation data on same train set which we going to compare which test data.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
def calculate_various_metrics(y_train,y_pred):
    # Calculate various metrics
    print(f'Accuracy: {accuracy_score(y_train, y_pred)}')
    print(f'Precision: {precision_score(y_train, y_pred)}')
    print(f'Recall: {recall_score(y_train, y_pred)}')
    print(f'F1 Score: {f1_score(y_train, y_pred)}')
    print(f'AUC-ROC: {roc_auc_score(y_train, y_pred)}')

    # Confusion matrix
    conf_matrix = confusion_matrix(y_train, y_pred)
    print('Confusion Matrix:')
    print(conf_matrix)

# Function to change threshold and evaluate the model
def evaluate_at_threshold(y_true, y_prob, threshold):
    y_pred = (y_prob >= threshold).astype(int)
    print(f"Threshold: {threshold}")
    print("Confusion Matrix:")
    print(confusion_matrix(y_true, y_pred))
    print("Classification Report:")
    print(classification_report(y_true, y_pred))
    print("AUC-ROC Score:")
    print(roc_auc_score(y_true, y_prob))
    print("\n")

# Evaluate the model on the testing set
y_pred = pipe.predict(X_train)
calculate_various_metrics(y_train,y_pred)

"""These results show our Model Perform very poorly this model fail to pred any True Negitive <br>
Our model is clearly underfit because of class imbalance. tuning hyperparameters, addressing class imbalance.

Using weight balance
"""

pipe = make_pipeline(column_trans_2,LogisticRegression(max_iter=10000,solver='liblinear', class_weight='balanced'))

#fitting model
pipe.fit(X_train,y_train)

# Evaluate the model on the testing set
y_pred = pipe.predict(X_train)

calculate_various_metrics(y_train,y_pred)

pipe = make_pipeline(column_trans_2,LogisticRegression(max_iter=10000,solver='liblinear',penalty='l2',C=0.001, class_weight='balanced'))


# Split the data into training and validation sets
X_train1, X_val, y_train1, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42,stratify=y_train)


# Fit the model on the training data
pipe.fit(X_train1, y_train1)

# Get predicted probabilities on the validation set
y_prob = pipe.predict_proba(X_val)[:, 1]  # Probability of positive class


# Evaluate at different thresholds
thresholds_to_test = [0.1,0.2,0.5, 0.6, 0.7]
for threshold in thresholds_to_test:
    evaluate_at_threshold(y_val, y_prob, threshold)

"""### Using Weight balance, Reguralization does't help much because of class imbalance size 99.5 by 0.5
So we going to apply undersample on majority class and under sampling on minority class
but first we going to import test dataset because we going to test our model how it behaves on unseen data.

Importing test dataset
"""

df_test = pd.read_csv('/content/drive/MyDrive/Fraud Detection/DataSet/archive (3)/fraudTest.csv')

print(df_test.shape)
df_test.head()

print(test_length := df_test['is_fraud'].value_counts())

print(f"Fraud row %: {test_length[1]/len(df_test)}")
print(f"Not Fraud row %: {test_length[0]/len(df_test)}")

df_test.isna().sum()

"""### Applying Same processes on Test data so they both equaly match in terms of shape and datatype."""

df_test = df_test[df_test['amt'] < 3000]
filtered_rows = df_test[(df_test["long"] < -130) & (df_test['is_fraud']==0)]
df_test.drop(filtered_rows.index, inplace=True)
df_test.drop(columns=['Unnamed: 0', 'cc_num','first', 'last', 'unix_time','trans_num'], inplace=True)
# feature = df_test.columns
desired_order =['merchant', 'category', 'street', 'city', 'state', 'job','trans_date_trans_time', 'amt',
                'gender', 'zip', 'lat', 'long', 'city_pop','dob', 'merch_lat', 'merch_long', 'is_fraud']
df_test = df_test[desired_order]

X = column_trans.transform(df_test)
X = pd.DataFrame(X,columns=df_test.columns)

df_test = X

# Extract time-based features
df_test['trans_hour'] = pd.to_datetime(df_test['trans_date_trans_time']).dt.hour
df_test['trans_day_of_week'] = pd.to_datetime(df_test['trans_date_trans_time']).dt.dayofweek

# 'dob' is the column containing date of birth
df_test['dob'] = pd.to_datetime(df_test['dob'])

# Calculate age based on the current date
current_date = datetime.now()
df_test['age'] = (current_date - df_test['dob']).dt.days // 365

df_test.drop(columns=['trans_date_trans_time', 'dob'], inplace=True)

HighRiskMerchant = df_test[df_test['is_fraud'] == 1]['merchant'].value_counts()
HighRiskMerchant = HighRiskMerchant[HighRiskMerchant > 20].index.tolist()

# Create 'high_risk_merchant' column
df_test['high_risk_merchant'] = np.where(df_test['merchant'].isin(HighRiskMerchant), 1, 0)

# Create high_risk_city, high_risk_state, high_risk_job
HighRiskCity =  df_test[df_test['is_fraud'] == 1]['city'].value_counts().index.tolist()
df_test['high_risk_city'] = np.where(df_test['city'].isin(HighRiskCity) & df_test['city_pop']>1000, 1, 0)
HighRiskState =  df_test[df_test['is_fraud'] == 1]['state'].value_counts().index.tolist()
df_test['high_risk_state'] =  np.where(df_test['state'].isin(HighRiskState) , 1, 0)
HighRiskJob =  df_test[df_test['is_fraud'] == 1]['job'].value_counts()
HighRiskJob = HighRiskJob[HighRiskJob > 10].index.tolist()
df_test['high_risk_job'] = np.where(df_test['job'].isin(HighRiskJob), 1, 0)

object_columns = df_test.select_dtypes(include='object').columns
int_columns = df_test.select_dtypes(include=['int32','int64','float32','float64']).columns
print('Object Type columns are:\n',object_columns := list(object_columns))
print('Int Type columns are:\n',int_columns := list(int_columns))
object_columns.remove('gender')

df_test[object_columns] = df_test[object_columns].astype(float)

df_test.head()

# Separate target variable and features
y_test = df_test['is_fraud']
X_test = df_test.drop(columns='is_fraud')
print(y_test.shape)
print(X_test.shape)

"""# Now Evalutating with test data
This test for reference for how much model improved after applying Class Balancing techniques.
"""

# Fit the model on the training data
pipe.fit(X_train, y_train)

# Get predicted probabilities on the validation set
y_prob = pipe.predict_proba(X_test)[:, 1]  # Probability of positive class


# Evaluate at different thresholds
thresholds_to_test = [0.1,0.2,0.5, 0.6, 0.7]
for threshold in thresholds_to_test:
    evaluate_at_threshold(y_test, y_prob, threshold)

# y_pred = pipe.predict(X_test)
# x = range(0,len(y_test))

# # Plot the actual vs. predicted values
# plt.scatter(x, y_test, c='blue', marker='o', alpha=0.5)
# plt.plot(x, y_pred, linestyle='--', color='red', linewidth=2)
# plt.xlabel('Actual Values')
# plt.ylabel('Predicted Values')
# plt.title('Actual vs. Predicted Values')
# plt.show()

y_pred = pipe.predict(X_test)
x = range(0,len(y_test))
fig,ax = plt.subplots(1,1,figsize=(5,3))
# Plot z vs sigmoid(z)
ax.scatter(x, y_test, c='blue', marker='o', alpha=0.5)
ax.plot(y_prob, y_pred, c="b")
ax.set_title("Sigmoid function")
ax.set_ylabel('sigmoid(z)')
ax.set_xlabel('z')

"""As predicted With too many columns Model prediction visualization can be very challenging.
Because there

# Performance Improvement:
"""

def evaluate_samply_data(X_train_resampled, y_train_resampled, X_test_resampled, y_test_resampled):
  # shape of resampled test train sample.
  print(f'X_train shape: {X_train_resampled.shape}\ny_train Shape: {y_train_resampled.shape}')
  print(f'X_test shape: {X_test_resampled.shape}\ny_test Shape: {y_test_resampled.shape}')
  # make pipeline
  pipe = make_pipeline(column_trans_2,LogisticRegression(max_iter=10000,solver='liblinear',penalty='l2',C=0.001, class_weight='balanced'))

  # Fit the model on the training data
  pipe.fit(X_train_resampled, y_train_resampled)

  # Get predicted probabilities on the validation set
  y_prob = pipe.predict_proba(X_test_resampled)[:, 1]  # Probability of positive class


  # Evaluate at different thresholds
  thresholds_to_test = [0.3, 0.4, 0.5, 0.6, 0.7]
  for threshold in thresholds_to_test:
      evaluate_at_threshold(y_test_resampled, y_prob, threshold)

def generate_auc_roc_curve(X_train_resampled, y_train_resampled,X_test_resampled ,y_test_resampled):

    pipe = make_pipeline(column_trans_2,LogisticRegression(max_iter=10000,solver='liblinear',penalty='l2',C=0.001, class_weight='balanced'))

    # Fit the model on the training data
    pipe.fit(X_train_resampled, y_train_resampled)

    # Get predicted probabilities on the validation set
    y_pred_proba1 = pipe.predict_proba(X_test_resampled)[:, 1]  # Probability of positive class
    fpr, tpr, thresholds = roc_curve(y_test_resampled,  y_pred_proba1)
    auc = roc_auc_score(y_test_resampled, y_pred_proba1)
    plt.plot(fpr,tpr,label="AUC ROC Curve with Area Under the curve ="+str(auc))
    plt.legend(loc=4)
    plt.show()
    pass

"""## 1. Random Under-sampling"""

#  Random Under-sampling
rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)
X_test_resampled, y_test_resampled = rus.fit_resample(X_test, y_test)

# Plotting bar for the distribution of classes in the resampled training set
plt.figure(figsize=(8, 6))
plt.bar(['Legitimate', 'Fraudulent'], [sum(y_train_resampled == 0), sum(y_train_resampled == 1)], color=['blue', 'red'])
plt.title('Distribution of Classes in Resampled Training Set')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

evaluate_samply_data(X_train_resampled, y_train_resampled, X_test_resampled, y_test_resampled)
generate_auc_roc_curve(X_train_resampled, y_train_resampled, X_test_resampled, y_test_resampled)

"""### Checking on Cross Validation"""

cv_accuracy = cross_val_score(pipe, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')
cv_f1_score = cross_val_score(pipe, X_train_resampled, y_train_resampled, cv=5, scoring='f1')
print(f'Model Average/generalized Accuracy: {cv_accuracy.mean()}\n Model Average/generalized F1-score: {cv_f1_score.mean()}')

"""### With Threshold: 0.4 Accuracy 0.85 and F1-score 0.85
Threshold: 0.4<br>
Confusion Matrix:
[[2054   91]
 [ 554 1591]]
Classification Report:
              precision    recall  f1-score   support

         0.0       0.79      0.96      0.86      2145
         1.0       0.95      0.74      0.83      2145

    accuracy                           0.85      4290
   macro avg       0.87      0.85      0.85      4290
weighted avg       0.87      0.85      0.85      4290

AUC-ROC Score:
0.8921742437826354

It's the best model so far

## 2. Random Over-sampling
"""

#  Random Over-sampling

ros = RandomOverSampler(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)
X_test_resampled, y_test_resampled = ros.fit_resample(X_test, y_test)

# Plotting bar for the distribution of classes in the resampled training set
plt.figure(figsize=(8, 6))
plt.bar(['Legitimate', 'Fraudulent'], [sum(y_train_resampled == 0), sum(y_train_resampled == 1)], color=['blue', 'red'])
plt.title('Distribution of Classes in Resampled Training Set')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

evaluate_samply_data(X_train_resampled, y_train_resampled, X_test_resampled, y_test_resampled)
generate_auc_roc_curve(X_train_resampled, y_train_resampled, X_test_resampled, y_test_resampled)

cv_accuracy = cross_val_score(pipe, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')
cv_f1_score = cross_val_score(pipe, X_train_resampled, y_train_resampled, cv=5, scoring='f1')
print(f'Model Average/generalized Accuracy: {cv_accuracy.mean()}\n Model Average/generalized F1-score: {cv_f1_score.mean()}')

"""Random Oversampling give a much more stable and Accurate results because it Train on 2.5 Million X and 1.1 Million y data.<br>
### Best result with this method is :
### With Threshold: 0.4 Accuracy 0.85 and F1-score 0.85
Threshold: 0.5 <br>
Confusion Matrix:
[[534720  16766]
 [138658 412828]]
Classification Report:
              precision    recall  f1-score   support

         0.0       0.79      0.97      0.87    551486
         1.0       0.96      0.75      0.84    551486

    accuracy                           0.86   1102972
AUC-ROC Score:
0.8837941374882067

## 3. SMOTE (Synthetic Minority Over-sampling Technique)
"""

#  SMOTE (Synthetic Minority Over-sampling Technique)
smote = SMOTE(sampling_strategy='auto', random_state=42 )

X1 = column_trans_2.transform(X_train)
X2 = column_trans_2.transform(X_test)

X_train_resampled, y_train_resampled = smote.fit_resample(X1, y_train)
X_test_resampled, y_test_resampled = smote.fit_resample(X2, y_test)

# Plotting bar for the distribution of classes in the resampled training set
plt.figure(figsize=(8, 6))
plt.bar(['Legitimate', 'Fraudulent'], [sum(y_train_resampled == 0), sum(y_train_resampled == 1)], color=['blue', 'red'])
plt.title('Distribution of Classes in Resampled Training Set')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

# shape of resampled test train sample.
print(f'X_train shape: {X_train_resampled.shape}\ny_train Shape: {y_train_resampled.shape}')
print(f'X_test shape: {X_test_resampled.shape}\ny_test Shape: {y_test_resampled.shape}')
# make pipeline
lg = LogisticRegression(max_iter=10000,solver='liblinear',penalty='l2',C=0.001, class_weight='balanced')

# Fit the model on the training data
lg.fit(X_train_resampled, y_train_resampled)

# Get predicted probabilities on the validation set
y_pred_proba = lg.predict_proba(X_test_resampled)[:, 1]  # Probability of positive class


# Evaluate at different thresholds
thresholds_to_test = [0.3, 0.4, 0.5, 0.6, 0.7]
for threshold in thresholds_to_test:
    evaluate_at_threshold(y_test_resampled, y_pred_proba, threshold)

fpr, tpr, thresholds = roc_curve(y_test_resampled,  y_pred_proba)
auc = roc_auc_score(y_test_resampled, y_pred_proba)
plt.plot(fpr,tpr,label="AUC ROC Curve with Area Under the curve ="+str(auc))
plt.legend(loc=4)
plt.show()

cv_accuracy = cross_val_score(lg, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')
cv_f1_score = cross_val_score(lg, X_train_resampled, y_train_resampled, cv=5, scoring='f1')
print(f'Model Average/generalized Accuracy: {cv_accuracy.mean()}\n Model Average/generalized F1-score: {cv_f1_score.mean()}')

"""Best result with this method is :
With Threshold: 0.5 Accuracy 0.87 and F1-score 0.85<br>
Threshold: 0.5
Confusion Matrix:
[[533367  18119]
 [130061 421425]]
Classification Report:
              precision    recall  f1-score   support

         0.0       0.80      0.97      0.88    551486
         1.0       0.96      0.76      0.85    551486

    accuracy                           0.87   1102972

AUC-ROC Score:
0.9142991983522012

## 4. SMOTE-ENN (SMOTE and Edited Nearest Neighbors)
"""

#  SMOTE-ENN (SMOTE and Edited Nearest Neighbors)

# The reason of using RamdomUnderSampler because it takes too much time to run.
rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)
X_test_resampled, y_test_resampled = rus.fit_resample(X_test, y_test)

smote_enn = SMOTEENN(sampling_strategy='auto', random_state=42)

X_train_resampled = column_trans_2.transform(X_train_resampled)
X_test_resampled = column_trans_2.transform(X_test_resampled)

X_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train_resampled, y_train_resampled)
X_test_resampled, y_test_resampled = smote_enn.fit_resample(X_test_resampled, y_test_resampled)

# Plotting bar for the distribution of classes in the resampled training set
plt.figure(figsize=(8, 6))
plt.bar(['Legitimate', 'Fraudulent'], [sum(y_train_resampled == 0), sum(y_train_resampled == 1)], color=['blue', 'red'])
plt.title('Distribution of Classes in Resampled Training Set')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()
# make pipeline
lg = LogisticRegression(max_iter=10000,solver='liblinear',penalty='l2',C=0.001, class_weight='balanced')

# Fit the model on the training data
lg.fit(X_train_resampled, y_train_resampled)

# Get predicted probabilities on the validation set
y_pred_proba = lg.predict_proba(X_test_resampled)[:, 1]  # Probability of positive class


# Evaluate at different thresholds
thresholds_to_test = [0.3, 0.4, 0.5, 0.6, 0.7]
for threshold in thresholds_to_test:
    evaluate_at_threshold(y_test_resampled, y_pred_proba, threshold)

fpr, tpr, thresholds = roc_curve(y_test_resampled,  y_pred_proba)
auc = roc_auc_score(y_test_resampled, y_pred_proba)
plt.plot(fpr,tpr,label="AUC ROC Curve with Area Under the curve ="+str(auc))
plt.legend(loc=4)
plt.show()

cv_accuracy = cross_val_score(lg, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')
cv_f1_score = cross_val_score(lg, X_train_resampled, y_train_resampled, cv=5, scoring='f1')
print(f'Model Average/generalized Accuracy: {cv_accuracy.mean()}\n Model Average/generalized F1-score: {cv_f1_score.mean()}')

"""Best result with this method is : With Threshold: 0.3 Accuracy 0.90 and F1-score 0.90<br>
Threshold: 0.3
Confusion Matrix:
[[1340   38]
 [ 250 1267]]
Classification Report:
              precision    recall  f1-score   support

         0.0       0.84      0.97      0.90      1378
         1.0       0.97      0.84      0.90      1517

    accuracy                           0.90      2895

AUC-ROC Score:
0.9635021761114719

## We find our best model

## Find our best sample balancing methed which is Smote-enn Which works with k-nearest neighbours oversampling.
Now we going  to find the best parameters for our model.
"""

# now finding best value for penalty term
C_value = np.linspace(0.001, 0.1, 10)
accuracy = []
f1_score = []
penalty_value = []
for c in C_value:
  lg = LogisticRegression(max_iter=10000,solver='liblinear',penalty='l2',C=c, class_weight='balanced')
  cv_accuracy = cross_val_score(lg, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')
  cv_f1_score = cross_val_score(lg, X_train_resampled, y_train_resampled, cv=5, scoring='f1')
  accuracy.append(cv_accuracy.mean())
  f1_score.append(cv_f1_score.mean())
  penalty_value.append(c)

score_dic = {"Model Accuracy":accuracy,
             "Model F1-Score":f1_score,
             "Penalty":penalty_value}
score_df = pd.DataFrame(score_dic)
score_df

"""## Change penalty term does't do much on Accuracy and F1_score so we find our best model which we going to save 95% train accuracy and 90% test accuracy."""

import joblib
model_lg = LogisticRegression(max_iter=10000,solver='liblinear',penalty='l2',C=c, class_weight='balanced')
# Evaluate at different thresholds
thresholds_to_test = [0.2, 0.3, 0.4, 0.5]
for threshold in thresholds_to_test:
    model_lg.fit(X_train_resampled,y_train_resampled)
    # Get predicted probabilities on the validation set
    y_pred_proba = model_lg.predict_proba(X_test_resampled)[:, 1]  # Probability of positive class
    evaluate_at_threshold(y_test_resampled, y_pred_proba, threshold)

# Save model in Pickle format
model_filename = f'/content/drive/MyDrive/Fraud Detection/my_model.joblib'  # Choose a suitable filenam
joblib.dump(model_lg, model_filename)  # Serialize the model to the file
print("Model saved successfully!")

fpr, tpr, thresholds = roc_curve(y_test_resampled,  y_pred_proba)
auc = roc_auc_score(y_test_resampled, y_pred_proba)
plt.plot(fpr,tpr,label="AUC ROC Curve with Area Under the curve ="+str(auc))
plt.legend(loc=4)
plt.show()

"""Althrough Model has best performance around 0.2 t0 0.3 threshold
but we are saving on this list just for reference [0.2, 0.3, 0.4, 0.5]

just to mention smote and smote-een use same method for random over sampling
the reason for smote-een is Hybrid (Oversampling + Undersampling).

## Just to take overview we are going to run all estimator
"""

import sklearn.metrics as sm
from sklearn.utils import all_estimators
estimators = all_estimators(type_filter='classifier')
model_name=[]
model_precision=[]
for name, get_model in estimators:
    try:
        model = get_model()
        model.fit(X_train_resampled,y_train_resampled)
        pred_y=model.predict(X_test_resampled)
        model_precision.append(sm.precision_score(y_test_resampled, pred_y))
        model_name.append(name)
    except Exception as e:
        print('Unable to import', name)
        print(e)

results=pd.DataFrame({"Model Name":model_name , "Model precision":model_precision})
results

# rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)
# X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)
# X_test_resampled, y_test_resampled = rus.fit_resample(X_test, y_test)

# #  SMOTE (Synthetic Minority Over-sampling Technique)
# smote = SMOTE(sampling_strategy='auto', random_state=42 )

# X_train_resampled = column_trans_2.transform(X_train_resampled)
# X_test_resampled = column_trans_2.transform(X_test_resampled)

# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_resampled, y_train_resampled)
# X_test_resampled, y_test_resampled = smote.fit_resample(X_test_resampled, y_test_resampled)

# pipe = make_pipeline( SMOTE(sampling_strategy='auto', random_state=42),LogisticRegression(max_iter=10000,solver='liblinear',penalty='l2',C=0.001, class_weight='balanced'))
# weights = np.linspace(0.005, 0.25, 10)

# gsc = GridSearchCV(
#     estimator=pipe,
#     param_grid={
#         'smote__sampling_strategy' : weights
#     },
#     scoring='f1',
#     cv=3
# )
# grid_result = gsc.fit(X_train_resampled, y_train_resampled)

# print("Best parameters : %s" % grid_result.best_params_)
# weight_f1_score_df = pd.DataFrame({ 'score': grid_result.cv_results_['mean_test_score'],
#                                    'weight': weights })
# weight_f1_score_df.plot(x='weight')